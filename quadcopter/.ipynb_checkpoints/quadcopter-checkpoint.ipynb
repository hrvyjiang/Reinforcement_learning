{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement learning with Quadcopter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Basic_Agent():\n",
    "    def __init__(self, task):\n",
    "        self.task = task\n",
    "    \n",
    "    def act(self):\n",
    "        new_thrust = random.gauss(450., 25.)\n",
    "        return [new_thrust + random.gauss(0., 1.) for x in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +=: 'int' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m     rotor_speeds \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact()\n\u001b[0;32m---> 30\u001b[0m     _, _, done \u001b[38;5;241m=\u001b[39m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrotor_speeds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     to_write \u001b[38;5;241m=\u001b[39m [task\u001b[38;5;241m.\u001b[39msim\u001b[38;5;241m.\u001b[39mtime] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(task\u001b[38;5;241m.\u001b[39msim\u001b[38;5;241m.\u001b[39mpose) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(task\u001b[38;5;241m.\u001b[39msim\u001b[38;5;241m.\u001b[39mv) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(task\u001b[38;5;241m.\u001b[39msim\u001b[38;5;241m.\u001b[39mangular_v) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(rotor_speeds)\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ii \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(labels)):\n",
      "File \u001b[0;32m~/Desktop/ML_Portfolio/Reinforcement_learning/quadcopter/tasks/basic.py:28\u001b[0m, in \u001b[0;36mTask.step\u001b[0;34m(self, roto_speeds)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_repeat):\n\u001b[1;32m     27\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msim\u001b[38;5;241m.\u001b[39mnext_timestep(roto_speeds)\n\u001b[0;32m---> 28\u001b[0m     reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_reward()\n\u001b[1;32m     29\u001b[0m     pose_all\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msim\u001b[38;5;241m.\u001b[39mpose)\n\u001b[1;32m     30\u001b[0m next_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(pose_all)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +=: 'int' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "from tasks.basic import Task\n",
    "\n",
    "runtime = 5.                                     # time limit of the episode\n",
    "init_pose = np.array([0., 0., 10., 0., 0., 0.])  # initial pose\n",
    "init_velocities = np.array([0., 0., 0.])         # initial velocities\n",
    "init_angle_velocities = np.array([0., 0., 0.])   # initial angle velocities\n",
    "file_output = 'data.txt'                         # file name for saved results\n",
    "\n",
    "#Setup\n",
    "task = Task(init_pose, init_velocities, init_angle_velocities, runtime)\n",
    "agent = Basic_Agent(task)\n",
    "done = False\n",
    "labels = ['time', 'x', 'y', 'z', 'phi', 'theta', 'psi', 'x_velocity',\n",
    "          'y_velocity','z_velocity','phi_velocity','theta_velocity',\n",
    "          'psi_velocity','roto_speed1','roto_speed2','roto_speed3','roto_speed4']\n",
    "results = {x : [] for x in labels}\n",
    "\n",
    "\n",
    "#Run the simulation and save result\n",
    "with open(file_output, 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(labels)\n",
    "    while True:\n",
    "        rotor_speeds = agent.act()\n",
    "        _, _, done = task.step(rotor_speeds)\n",
    "        to_write = [task.sim.time] + list(task.sim.pose) + list(task.sim.v) + list(task.sim.angular_v) + list(rotor_speeds)\n",
    "        for ii in range(len(labels)):\n",
    "            results[labels[ii]].append(to_write[ii])\n",
    "        writer.writerow(to_write)\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(results['time'], results['x'], label='x')\n",
    "plt.plot(results['time'], results['y'], label='y')\n",
    "plt.plot(results['time'], results['z'], label='z')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(results['time'], results['x_velocity'], label='x_hat')\n",
    "plt.plot(results['time'], results['y_velocity'], label='y_hat')\n",
    "plt.plot(results['time'], results['z_velocity'], label='z_hat')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(results['time'], results['phi'], label='phi')\n",
    "plt.plot(results['time'], results['theta'], label='theta')\n",
    "plt.plot(results['time'], results['psi'], label='psi')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(results['time'], results['phi_velocity'], label='phi_velocity')\n",
    "plt.plot(results['time'], results['theta_velocity'], label='theta_velocity')\n",
    "plt.plot(results['time'], results['psi_velocity'], label='psi_velocity')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import agents & tasks\n",
    "from agents.policy_search import PolicySearch_Agent\n",
    "from agents.ddpg import DDPG_Agent\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "num_episodes = 1000\n",
    "target_pos = np.array([0., 0., 0.])\n",
    "#task = Task(target_pos=target_pos)\n",
    "task = Task(target_pos=target_pos)\n",
    "agent = PolicySearch_Agent(task) \n",
    "\n",
    "for i_episode in range(1, num_episodes+1):\n",
    "    state = agent.reset_episode() # start a new episode\n",
    "    while True:\n",
    "        action = agent.act(state) \n",
    "        next_state, reward, done = task.step(action)\n",
    "        agent.step(reward, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"\\rEpisode = {:4d}, score = {:7.3f} (best = {:7.3f}), noise_scale = {}\".format(\n",
    "                i_episode, agent.score, agent.best_score, agent.noise_scale), end=\"\")  # [debug]\n",
    "            break\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Task, Design the Agent, and Train the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from tasks.basic import Task\n",
    "from tasks.takeoff import Task_takeoff\n",
    "\n",
    "exportPath = './simulation/'\n",
    "if not os.path.exists(exportPath):\n",
    "    os.makedirs(exportPath)\n",
    "\n",
    "# z axis is up\n",
    "init_pose = np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0]) \n",
    "target_pos = np.array([0.0, 0.0, 10.0])\n",
    "task = Task_takeoff(init_pose=init_pose, target_pos=target_pos, runtime=5.0)\n",
    "agent = DDPG_Agent(task)\n",
    "# number of episodes to train\n",
    "num_episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before training\n",
    "resultsAll = []\n",
    "high_score = -1000000.0\n",
    "low_score = 1000000.0\n",
    "\n",
    "training_results = {'score': [],'episode': []}\n",
    "for i_episode in range(1, num_episodes+1):\n",
    "    # start a new episode\n",
    "    state = agent.reset_episode() \n",
    "    score = 0\n",
    "\n",
    "    episode_results = {\n",
    "        'time': [],\n",
    "        'x': [],\n",
    "        'y': [],\n",
    "        'z': [],\n",
    "        'phi': [],\n",
    "        'theta': [],\n",
    "        'psi': [],\n",
    "        'vx': [],\n",
    "        'vy': [],\n",
    "        'vz': [],\n",
    "        'reward': [],\n",
    "        }\n",
    "\n",
    "    while True:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done = task.step(action)\n",
    "        agent.step(action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        high_score = max(high_score, score)\n",
    "        low_score = min(low_score, score)\n",
    "\n",
    "        # track the results for offline analysis\n",
    "        episode_results['time'].append(task.sim.time)\n",
    "        episode_results['x'].append(state[0])\n",
    "        episode_results['y'].append(state[1])\n",
    "        episode_results['z'].append(state[2])\n",
    "        episode_results['phi'].append(state[3])\n",
    "        episode_results['theta'].append(state[4])\n",
    "        episode_results['psi'].append(state[5])\n",
    "        episode_results['vx'].append(state[6])\n",
    "        episode_results['vy'].append(state[7])\n",
    "        episode_results['vz'].append(state[8])\n",
    "        episode_results['reward'].append(reward)\n",
    "        \n",
    "        if done:\n",
    "            print(\"\\rEpisode = {:4d}, score = {:7.3f}, low score = {:7.3f}, high score = {:7.3f}\".format(i_episode, score, low_score, high_score), end=\"\")\n",
    "            training_results['episode'].append(i_episode)\n",
    "            training_results['score'].append(score)\n",
    "            break\n",
    "\n",
    "    resultsAll.append(episode_results)\n",
    "\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# save results for later analysis\n",
    "with open(\"{}results.bin\".format(exportPath), 'wb') as pickleFile:\n",
    "    pickle.dump(resultsAll, pickleFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Render top result\n",
    "with open(\"{}results.bin\".format(exportPath), 'rb') as pickleFile:\n",
    "    results = pickle.load(pickleFile)\n",
    "\n",
    "# Sort wrt episode reward\n",
    "episodeRewards = [np.sum(r['reward']) for r in results]\n",
    "resultIndices = np.argsort(episodeRewards)\n",
    "\n",
    "# Get top results\n",
    "resultIndices = list(reversed(resultIndices))[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot reward for top result\n",
    "for iE, e in enumerate(resultIndices):\n",
    "    res = results[e]\n",
    "    \n",
    "    # plot reward\n",
    "    plt.plot(res['time'], res['reward'], label='reward')\n",
    "    plt.legend()\n",
    "    _ = plt.ylim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate visualisation\n",
    "# Based on: https://github.com/craig-martinson/quadcopter-project/blob/master/visualise.py\n",
    "from visualise import Quadrotor\n",
    "from IPython.display import Image\n",
    "import os\n",
    "import imageio\n",
    "\n",
    "images = []\n",
    "\n",
    "for iE, e in enumerate(resultIndices):\n",
    "    res = results[e] \n",
    "   \n",
    "    filepath = \"{}frame{:04}_{:04}.png\".format(exportPath, iE, 0)\n",
    "    \n",
    "    q = Quadrotor(x=res['x'][0], \n",
    "            y=res['y'][0], \n",
    "            z=res['z'][0], \n",
    "            roll=res['phi'][0],\n",
    "            pitch=res['theta'][0], \n",
    "            yaw=res['psi'][0],\n",
    "            reward=res['reward'][0], \n",
    "            title=None,\n",
    "            filepath=filepath)\n",
    "    \n",
    "    q.set_target(target_pos[0], target_pos[1], target_pos[2])\n",
    "    \n",
    "    # Render all frames in this episode\n",
    "    for i in range(1, len(res['x'])):\n",
    "        filepath = \"{}frame{:04}_{:04}.png\".format(exportPath, iE, i)\n",
    "        \n",
    "        q.update_pose(x=res['x'][i], \n",
    "        y=res['y'][i], \n",
    "        z=res['z'][i], \n",
    "        roll=res['phi'][i],\n",
    "        pitch=res['theta'][i], \n",
    "        yaw=res['psi'][i],\n",
    "        reward=res['reward'][i],\n",
    "        title=None,\n",
    "        filepath=filepath)\n",
    "        \n",
    "        images.append(imageio.imread(filepath))\n",
    "        \n",
    "    q.close()   \n",
    "    \n",
    "# Save all frames to animated gif\n",
    "imageio.mimsave(\"{}flight.gif\".format(exportPath), images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render animated gif\n",
    "with open(\"{}flight.gif\".format(exportPath), 'rb') as f:\n",
    "    display(Image(data=f.read(), format='png'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quadcop",
   "language": "python",
   "name": "quadcop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
