{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy storage optimization with RL in MicroGrids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the section below, you must run your methodology for solving the problem from start to finish :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pymgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pymgrid import Microgrid\n",
    "\n",
    "with open('data/building_1.pkl', 'rb') as f:\n",
    "    building_1 = pickle.load(f)\n",
    "\n",
    "with open('data/building_2.pkl', 'rb') as f:\n",
    "    building_2 = pickle.load(f)\n",
    "    \n",
    "with open('data/building_3.pkl', 'rb') as f:\n",
    "    building_3 = pickle.load(f)\n",
    "\n",
    "buildings = [building_1, building_2, building_3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Simple\" Reinforcement Learning based approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time # Evaluate frugality\n",
    "import json # Storing results\n",
    "import DiscreteEnvironment as DiscreteEnvironment # Imposed Discrete Environment\n",
    "import DiscreteEnvironment_modified as DiscreteEnvironment_modified # Imposed Discrete Environment\n",
    "import Agent as Agent # Defined Agent class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Agent & Environment Setup before your training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_environments = [\n",
    "    DiscreteEnvironment_modified.Environment(env_config={'building':buildings[i]}) for i in range(3)\n",
    "]\n",
    "\"\"\"\n",
    "Agent, potential Q-Table & other necessary setup code here \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_qtable(env, nb_action, building):\n",
    "    \"\"\"\n",
    "    Initialize the Q-table of the Q-learning.\n",
    "    The state is an intersection between net load possibilities (from -(pv production) to load),\n",
    "    and the state of charge of the battery (from soc_min and soc_max).\n",
    "\n",
    "    -- Input :\n",
    "        env : Enivronment object\n",
    "        nb_action : Number of action can be taken by each state during the Q-learning [Integer]\n",
    "\n",
    "    -- Ouput :\n",
    "        Q : A dict of state containing the value of weight of each action for each state\n",
    "            (Dict of Dict)\n",
    "    \"\"\"\n",
    "\n",
    "    state = []\n",
    "    Q = {}\n",
    "\n",
    "    # --- Defining state possibilities ---------------------------\n",
    "    for i in range(-int(env.parameters[\"PV_rated_power\"] - 1), int(env.parameters[\"load\"] + 2)):\n",
    "        for j in np.arange(round(env.battery.soc_min, 1), round(env.battery.soc_max + 0.1, 1), 0.1):\n",
    "            state.append((i, round(j, 1)))\n",
    "\n",
    "    # --- Initialize Q(s, a) at zero ----------------------------\n",
    "    for s in state:\n",
    "        Q[s] = {}\n",
    "        for a in range(nb_action):\n",
    "            if building==3:\n",
    "                if a==4:\n",
    "                    Q[s][a] = -50\n",
    "                elif a==6:\n",
    "                    Q[s][a] = -30\n",
    "                else:\n",
    "                    Q[s][a] = 0\n",
    "            else:\n",
    "                Q[s][a] = 0\n",
    "\n",
    "    return Q\n",
    "\n",
    "def epsilon_decreasing_greedy(action, epsilon, nb_action):\n",
    "    \"\"\"\n",
    "    Adding random aleas for the choice of actions\n",
    "\n",
    "    -- Input :\n",
    "        action : integer representing the action taken [Integer]\n",
    "        epsilon : share of aleas to consider (biggest espsilon is and biggest part of\n",
    "                  aleas is taken [Float]\n",
    "        nb_action : Number of action can be taken by each state during the Q-learning [Integer]\n",
    "\n",
    "    -- Output :\n",
    "        action : integer representing the action taken [Integer]\n",
    "        randomm : binary value to consider if a aleas has been taken for action choice [Integer]\n",
    "    \"\"\"\n",
    "    p = np.random.random()\n",
    "\n",
    "    if p < (1 - epsilon):\n",
    "        randomm = 0\n",
    "        return action, randomm\n",
    "\n",
    "    else:\n",
    "        randomm = 1\n",
    "        return np.random.choice(nb_action), randomm\n",
    "    \n",
    "def max_dict(d):\n",
    "    \"\"\"\n",
    "    Returning the tuple (action, val) maximizing the reward in the Q-table depending of a state.\n",
    "    Reward correspond to the amount paid to answer the consumption (count negatively)\n",
    "    => Maximizing a negative number = Minimizing its absolute value\n",
    "\n",
    "    -- Input :\n",
    "        d : dict of action and reward associate of a state [Dict]\n",
    "\n",
    "    -- Output :\n",
    "        max_key : action corresponding to the maximal reward [Integer]\n",
    "        max_value : value of the maximal reward [Integer]\n",
    "    \"\"\"\n",
    "    max_key = None\n",
    "    max_val = float(\"-inf\")\n",
    "\n",
    "    for k, v in d.items():\n",
    "        if v > max_val:\n",
    "            max_val = v\n",
    "            max_key = k\n",
    "\n",
    "    return max_key, max_val\n",
    "\n",
    "def update_epsilon(epsilon):\n",
    "    \"\"\"\n",
    "    Update epsilon value (share of aleas in the choice of an action) to minimize it through iteration.\n",
    "\n",
    "    -- Input :\n",
    "        epsilon : share of aleas to consider (biggest espsilon is and biggest part of\n",
    "                  aleas is taken [Float]\n",
    "\n",
    "    -- Output :\n",
    "        epsilon (updated) : share of aleas to consider (biggest espsilon is and biggest part of\n",
    "                            aleas is taken [Float]\n",
    "    \"\"\"\n",
    "    epsilon = epsilon - epsilon * 0.02\n",
    "\n",
    "    if epsilon < 0.1:\n",
    "        epsilon = 0.1\n",
    "\n",
    "    return epsilon\n",
    "\n",
    "def change_name_action(idx, building):\n",
    "    \"\"\"\n",
    "    Print function\n",
    "    \"\"\"\n",
    "    if building==3:\n",
    "        if idx == 0:\n",
    "            action_name = \"PV + Charge + Export\"\n",
    "        elif idx == 5:\n",
    "            action_name = \"PV + Discharge + Import\"\n",
    "        elif idx == 2:\n",
    "            action_name = \"Import\"\n",
    "        elif idx == 3:\n",
    "            action_name = \"Full Export\"\n",
    "        elif idx == 4:\n",
    "            action_name = \"Genset\"\n",
    "        elif idx == 1:\n",
    "            action_name = \"Export/Import\"\n",
    "        elif idx == 6:\n",
    "            action_name = \"Genset\"\n",
    "    else:\n",
    "        if idx == 0:\n",
    "            action_name = \"PV + Charge + Export\"\n",
    "        elif idx == 1:\n",
    "            action_name = \"PV + Discharge + Import\"\n",
    "        elif idx == 2:\n",
    "            action_name = \"Import\"\n",
    "        elif idx == 3:\n",
    "            action_name = \"Full Export\"\n",
    "        elif idx == 4:\n",
    "            action_name = \"Export/Import\"\n",
    "    \n",
    "    return action_name\n",
    "\n",
    "def print_welcome(idx):\n",
    "    \"\"\"\n",
    "    Print function\n",
    "    \"\"\"\n",
    "    if idx == 0:\n",
    "        print(\"------------------------------------\")\n",
    "        print(\"|        WELCOME TO PYMGRID        |\")\n",
    "        print(\"------------------------------------\")\n",
    "    elif idx == 1:\n",
    "\n",
    "        print(\"t -     STATE  -  ACTION - COST\")\n",
    "        print(\"================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_Q_Learning_DE(env, nb_action, building, horizon):\n",
    "\n",
    "    # --- Defining parameters ----------------------------------\n",
    "    Q = init_qtable(env.mg, nb_action, building)\n",
    "    nb_state = len(Q)\n",
    "    nb_episode = 15\n",
    "    alpha = 0.1    #  Learning rate\n",
    "    epsilon = 0.1  #  Aleas\n",
    "    gamma = 0.99\n",
    "    record_cost = []\n",
    "    t0 = time.time()\n",
    "    t = t0\n",
    "    print_training = \"Training Progressing .   \"\n",
    "    print_welcome(0)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    for e in range(nb_episode + 1):\n",
    "\n",
    "        # --- Initialize episode variables --------------------------\n",
    "        episode_cost = 0\n",
    "        env.reset()\n",
    "        net_load = round(env.mg.load - env.mg.pv)\n",
    "        soc = round(env.mg.battery.soc, 1)\n",
    "        s = (net_load, soc)  # First state\n",
    "        a = max_dict(Q[s])[0]  # First action\n",
    "        a, randomm = epsilon_decreasing_greedy(a, epsilon, nb_action)  # Adding aleas in the first action\n",
    "\n",
    "        # --- Q-learning accros horizon ------------------------------\n",
    "        for i in range(horizon):\n",
    "\n",
    "            # Run action choosen precedently\n",
    "            status, reward, done, info = env.step(a)\n",
    "            \n",
    "            # Compute cost with the previous actions\n",
    "            r = reward\n",
    "            episode_cost = env.get_cost()\n",
    "\n",
    "            # Update variables depending on the precedent action\n",
    "            net_load = round(env.mg.load - env.mg.pv)\n",
    "            soc = round(env.mg.battery.soc, 1)\n",
    "            s_ = (net_load, soc)\n",
    "            a_ = max_dict(Q[s_])[0]\n",
    "\n",
    "            if i == horizon - 1:\n",
    "                Q[s][a] += alpha * (r - Q[s][a])\n",
    "\n",
    "            # Update reward depending on the action choosen\n",
    "            else:\n",
    "                old_Q = Q[s][a]  # Previous reward\n",
    "                target = (r + gamma * Q[s_][a_])  # Target = reward of the previous action + expectation of reward of the last action\n",
    "                td_error = target - Q[s][a]  # Difference of cost between two episode\n",
    "                Q[s][a] = (1 - alpha) * Q[s][a] + alpha * td_error  # Update weight in the Q-table with the reward of the last action\n",
    "            s, a = s_, a_\n",
    "        epsilon = update_epsilon(epsilon)\n",
    "\n",
    "    return Q\n",
    "\n",
    "def testing_Q_Learning_DE(env, Q, horizon, building):\n",
    "\n",
    "    # --- Initialize variables --------------------------\n",
    "    env.reset()\n",
    "    net_load = round(env.mg.load - env.mg.pv)\n",
    "    soc = round(env.mg.battery.soc, 1)\n",
    "    s = (net_load, soc)\n",
    "    a = max_dict(Q[s])[0]\n",
    "    total_cost = 0\n",
    "    print_welcome(1)\n",
    "\n",
    "    # --- Q-learning accros horizon ----------------------\n",
    "    for i in range(horizon):\n",
    "\n",
    "        # Run action choosen precedently\n",
    "        action_name = change_name_action(a, building)        \n",
    "        status, reward, done, info = env.step(a)\n",
    "        cost = - reward\n",
    "        total_cost = env.get_cost()\n",
    "\n",
    "        # Print function\n",
    "        #if i % 500 == 0 or i == horizon - 1:\n",
    "        #print(i, \" -\", (int(net_load), soc), action_name, round(total_cost, 1), \"€\")\n",
    "\n",
    "        #  Update variables depending on the last action\n",
    "        net_load = round(env.mg.load - env.mg.pv)\n",
    "        soc = round(env.mg.battery.soc, 1)\n",
    "\n",
    "        #  Defining the next state and action corresponding\n",
    "        s_ = (net_load, soc)\n",
    "        a_ = max_dict(Q[s_])[0]\n",
    "        s, a = s_, a_\n",
    "        \n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training of the agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_start = time.process_time()\n",
    "\n",
    "\"\"\"\n",
    "Training code\n",
    "\"\"\"\n",
    "Q0_DE = training_Q_Learning_DE(building_environments[0], 5, 1, 8757)\n",
    "Q1_DE = training_Q_Learning_DE(building_environments[1], 5, 2, 8757)\n",
    "Q2_DE = training_Q_Learning_DE(building_environments[2], 7, 3, 8757)\n",
    "\n",
    "train_end = time.process_time()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frugality = train_end - train_start\n",
    "print(train_frugality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test of the agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_start = time.process_time()\n",
    "total_cost = [0,0,0]\n",
    "\n",
    "building_environments[0].reset(testing=True)\n",
    "total_cost[0] = testing_Q_Learning_DE(building_environments[0], Q0_DE, 8757, 1)\n",
    "\n",
    "building_environments[1].reset(testing=True)\n",
    "total_cost[1] = testing_Q_Learning_DE(building_environments[1], Q1_DE, 8757, 2)\n",
    "\n",
    "building_environments[2].reset(testing=True)\n",
    "total_cost[2] = testing_Q_Learning_DE(building_environments[2], Q2_DE, 8757, 3)\n",
    "    \n",
    "test_end = time.process_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Note :\n",
    "* To make your work as reproductible as possible, have a full-greedy approach (no exploration) on the test buildings\n",
    "* If your algorithm has some unavoidable randomness, consider running it for many loops and return a\n",
    "  mean profitability and mean frugality\n",
    "  \n",
    "\"\"\"\n",
    "\n",
    "test_start = time.process_time()\n",
    "total_cost = [0,0,0]\n",
    "\n",
    "for i,building_env in enumerate(building_environments):\n",
    "    \n",
    "    obs = building_env.reset(testing=True)\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = np.random.randint(building_env.action_space.n)\n",
    "        obs, reward, done, info = building_env.step(action)\n",
    "        total_cost[i]+=reward\n",
    "\n",
    "test_end = time.process_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_frugality = test_end - test_start\n",
    "print(test_frugality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Store & Export Results in JSON format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = {\n",
    "    \"building_1_performance\" : total_cost[0],\n",
    "    \"building_2_performance\" : total_cost[1],\n",
    "    \"building_3_performance\" : total_cost[2],\n",
    "    \"frugality\" : train_frugality + test_frugality,\n",
    "}\n",
    "print(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_name = 'team35'\n",
    "\n",
    "with open('results/' + team_name + '.txt', 'w') as json_file:\n",
    "    json.dump(final_results, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Reinforcement Learning based approache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time # Necessary to evaluate frugality\n",
    "from pymgrid.Environments.pymgrid_cspla import MicroGridEnv # Imposed Environment\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "## Import your favourite Deep Learning library for RL and other packages here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Agent & Environment Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Below is an environment initialization without a Deep RL library, the code can vary depending on which library you \n",
    "use\n",
    "\"\"\"\n",
    "building_environments = [MicroGridEnv(env_config={'microgrid':buildings[i]}) for i in range(3)]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deploying the agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(gamma=0.99, epsilon=1.0, batch_size=128, nb_actions=5,\n",
    "             eps_end=0.01, input_dims=[10], lr= 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start = time.process_time()\n",
    "\n",
    "nb_episode = 15\n",
    "\n",
    "for idx, env in enumerate(building_environments):\n",
    "    print(f\"building {idx}\")\n",
    "    for i in range(nb_episode):\n",
    "        score = 0\n",
    "        done = False\n",
    "        observation = env.reset()\n",
    "        while not done:\n",
    "            action = agent.choose_action(observation)\n",
    "            observation_, reward, done, info = env.step(action)\n",
    "            score -= reward\n",
    "            agent.store_transition(observation, action, reward, observation_, done)\n",
    "            agent.train()\n",
    "            observation = observation_\n",
    "        print(f\"- episode : {i} | score : {score}\")\n",
    "train_end = time.process_time()\n",
    "train_frugality = train_end - train_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_frugality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test of the agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Note :\n",
    "* To make your work as reproductible as possible, have a full-greedy approach (no exploration) on the test buildings\n",
    "* If your algorithm has some unavoidable randomness, consider running it for many loops and return a\n",
    "mean profitability and mean frugality\n",
    "\"\"\"\n",
    "\n",
    "test_start = time.process_time()\n",
    "\n",
    "test_start = time.process_time()\n",
    "total_cost = [0,0,0]\n",
    "\n",
    "for i,building_env in enumerate(building_environments):\n",
    "\n",
    "    obs = building_env.reset(testing=True)\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.choose_action(obs)\n",
    "        obs, reward, done, info = building_env.step(action)\n",
    "        total_cost[i]-=reward\n",
    "\n",
    "test_end = time.process_time()\n",
    "\n",
    "test_frugality = test_end - test_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Store & Export Results in JSON format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = {\n",
    "    \"building_1_performance\" : total_cost[0],\n",
    "    \"building_2_performance\" : total_cost[1],\n",
    "    \"building_3_performance\" : total_cost[2],\n",
    "    \"frugality\" : train_frugality + test_frugality,\n",
    "}\n",
    "print(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('results/' + '.txt', 'w') as json_file:\n",
    "    json.dump(final_results, json_file)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "microgrid",
   "language": "python",
   "name": "microgrid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
